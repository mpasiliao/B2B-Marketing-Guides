{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2B Marketing Analytics: Hands-On Exercises\n",
    "\n",
    "This notebook accompanies the B2B Marketing Guide Series. You'll work with a realistic synthetic dataset to practice:\n",
    "\n",
    "- Data cleaning and preparation\n",
    "- Conversion rate analysis\n",
    "- Attribution modeling\n",
    "- Cohort analysis\n",
    "- Pipeline reporting\n",
    "\n",
    "**Prerequisites:** Familiarity with concepts from Guides 1-3 (B2B fundamentals, martech stack, attribution models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Data Generation\n",
    "\n",
    "First, we generate a realistic synthetic B2B dataset. The data reflects typical B2B patterns:\n",
    "- 90-day average sales cycle\n",
    "- ~20% MQL→SQL conversion\n",
    "- ~25% SQL→Opportunity conversion  \n",
    "- ~30% Opportunity→Closed Won\n",
    "- Intentional data quality issues (duplicates, missing values, inconsistent formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NUM_LEADS = 5000\n",
    "NUM_ACCOUNTS = 800\n",
    "START_DATE = datetime(2023, 1, 1)\n",
    "END_DATE = datetime(2024, 6, 30)\n",
    "\n",
    "# Realistic company data\n",
    "COMPANY_PREFIXES = ['Tech', 'Data', 'Cloud', 'Cyber', 'Smart', 'Digital', 'Global', 'Prime', 'Next', 'Core',\n",
    "                    'Alpha', 'Beta', 'Apex', 'Peak', 'Vertex', 'Nova', 'Quantum', 'Fusion', 'Synergy', 'Nexus']\n",
    "COMPANY_SUFFIXES = ['Solutions', 'Systems', 'Technologies', 'Labs', 'Analytics', 'Software', 'Dynamics',\n",
    "                    'Innovations', 'Ventures', 'Partners', 'Group', 'Inc', 'Corp', 'Co', 'Industries']\n",
    "INDUSTRIES = ['Technology', 'Financial Services', 'Healthcare', 'Manufacturing', 'Retail', \n",
    "              'Professional Services', 'Education', 'Media', 'Telecommunications', 'Energy']\n",
    "\n",
    "LEAD_SOURCES = ['paid_search', 'linkedin', 'organic', 'referral', 'event', 'content_syndication']\n",
    "LEAD_SOURCE_WEIGHTS = [0.25, 0.20, 0.15, 0.10, 0.15, 0.15]\n",
    "\n",
    "# Self-reported sources (messy, realistic free-text responses)\n",
    "SELF_REPORTED_TEMPLATES = {\n",
    "    'paid_search': ['Google search', 'google', 'searched online', 'Google', 'web search', 'Googled it', \n",
    "                    'found you on google', 'internet search', None],\n",
    "    'linkedin': ['LinkedIn', 'linkedin', 'LinkedIn ad', 'saw your post on LinkedIn', 'LI', \n",
    "                 'linkedin article', 'a linkedin post', None],\n",
    "    'organic': ['blog post', 'your blog', 'article I read', 'content on your site', 'organic search',\n",
    "                'found your website', 'SEO', None, None],  # More Nones = more missing\n",
    "    'referral': ['colleague recommendation', 'friend told me', 'word of mouth', 'my boss mentioned you',\n",
    "                 'referral from a friend', 'heard about you from someone', 'a peer recommended you', None],\n",
    "    'event': ['conference', 'webinar', 'your webinar last month', 'trade show', 'SaaStr', 'Dreamforce',\n",
    "              'met at an event', 'virtual event', None],\n",
    "    'content_syndication': ['downloaded a whitepaper', 'ebook', 'read your report', 'G2', 'TrustRadius',\n",
    "                            'content download', None, None, None]  # Often no attribution\n",
    "}\n",
    "\n",
    "# Dark funnel additions - sometimes people report sources that don't match tracking\n",
    "DARK_FUNNEL_SOURCES = ['podcast', 'heard on a podcast', 'YouTube video', 'Twitter', 'community slack',\n",
    "                       'newsletter', 'been following you for a while', \"can't remember\", 'not sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_company_name():\n",
    "    \"\"\"Generate realistic company names with some duplicates and variations.\"\"\"\n",
    "    prefix = random.choice(COMPANY_PREFIXES)\n",
    "    suffix = random.choice(COMPANY_SUFFIXES)\n",
    "    return f\"{prefix} {suffix}\"\n",
    "\n",
    "def generate_email(name, company):\n",
    "    \"\"\"Generate email with realistic variations.\"\"\"\n",
    "    company_domain = company.lower().replace(' ', '').replace(',', '')[:12]\n",
    "    domains = [f\"{company_domain}.com\", f\"{company_domain}.io\", f\"{company_domain}.co\"]\n",
    "    name_parts = name.lower().split()\n",
    "    \n",
    "    formats = [\n",
    "        f\"{name_parts[0]}.{name_parts[1]}@{random.choice(domains)}\",\n",
    "        f\"{name_parts[0][0]}{name_parts[1]}@{random.choice(domains)}\",\n",
    "        f\"{name_parts[0]}@{random.choice(domains)}\"\n",
    "    ]\n",
    "    return random.choice(formats)\n",
    "\n",
    "def random_date(start, end):\n",
    "    \"\"\"Generate random date between start and end.\"\"\"\n",
    "    delta = end - start\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    return start + timedelta(days=random_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Accounts\n",
    "accounts = []\n",
    "company_names_generated = set()\n",
    "\n",
    "for i in range(NUM_ACCOUNTS):\n",
    "    company_name = generate_company_name()\n",
    "    # Allow some duplicate names (realistic)\n",
    "    if random.random() < 0.95:\n",
    "        while company_name in company_names_generated:\n",
    "            company_name = generate_company_name()\n",
    "    company_names_generated.add(company_name)\n",
    "    \n",
    "    employee_count = int(np.random.lognormal(6, 1.5))  # Skewed toward smaller companies\n",
    "    employee_count = min(max(employee_count, 10), 50000)\n",
    "    \n",
    "    # Revenue correlates with employees\n",
    "    revenue_per_employee = random.uniform(80000, 300000)\n",
    "    annual_revenue = int(employee_count * revenue_per_employee)\n",
    "    \n",
    "    # Tier based on company size\n",
    "    if employee_count >= 1000 or annual_revenue >= 100_000_000:\n",
    "        tier = 1\n",
    "    elif employee_count >= 200 or annual_revenue >= 20_000_000:\n",
    "        tier = 2\n",
    "    else:\n",
    "        tier = 3\n",
    "    \n",
    "    accounts.append({\n",
    "        'account_id': f'ACC-{i+1:04d}',\n",
    "        'company_name': company_name,\n",
    "        'industry': random.choice(INDUSTRIES),\n",
    "        'employee_count': employee_count,\n",
    "        'annual_revenue': annual_revenue,\n",
    "        'tier': tier\n",
    "    })\n",
    "\n",
    "accounts_df = pd.DataFrame(accounts)\n",
    "print(f\"Generated {len(accounts_df)} accounts\")\n",
    "print(f\"Tier distribution: {accounts_df['tier'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Leads\n",
    "FIRST_NAMES = ['James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael', 'Linda', 'William', 'Elizabeth',\n",
    "               'David', 'Barbara', 'Richard', 'Susan', 'Joseph', 'Jessica', 'Thomas', 'Sarah', 'Christopher', 'Karen',\n",
    "               'Daniel', 'Lisa', 'Matthew', 'Nancy', 'Anthony', 'Betty', 'Mark', 'Margaret', 'Donald', 'Sandra',\n",
    "               'Steven', 'Ashley', 'Paul', 'Kimberly', 'Andrew', 'Emily', 'Joshua', 'Donna', 'Kenneth', 'Michelle']\n",
    "LAST_NAMES = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez',\n",
    "              'Hernandez', 'Lopez', 'Gonzalez', 'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin',\n",
    "              'Lee', 'Perez', 'Thompson', 'White', 'Harris', 'Sanchez', 'Clark', 'Ramirez', 'Lewis', 'Robinson']\n",
    "\n",
    "leads = []\n",
    "contact_to_account = []\n",
    "\n",
    "for i in range(NUM_LEADS):\n",
    "    # Assign to account (some leads, especially from larger accounts, share accounts)\n",
    "    account = accounts_df.sample(1, weights=(accounts_df['employee_count'] ** 0.3)).iloc[0]\n",
    "    \n",
    "    first_name = random.choice(FIRST_NAMES)\n",
    "    last_name = random.choice(LAST_NAMES)\n",
    "    full_name = f\"{first_name} {last_name}\"\n",
    "    \n",
    "    # Company name variations (data quality issue)\n",
    "    company_name_variations = [\n",
    "        account['company_name'],\n",
    "        account['company_name'].upper(),\n",
    "        account['company_name'].lower(),\n",
    "        account['company_name'].replace(' ', ''),\n",
    "        account['company_name'] + ', Inc.',\n",
    "        account['company_name'] + ' LLC'\n",
    "    ]\n",
    "    company_name = random.choices(\n",
    "        company_name_variations, \n",
    "        weights=[0.7, 0.05, 0.05, 0.05, 0.1, 0.05]\n",
    "    )[0]\n",
    "    \n",
    "    lead_source = random.choices(LEAD_SOURCES, weights=LEAD_SOURCE_WEIGHTS)[0]\n",
    "    \n",
    "    # Self-reported source (messy, with dark funnel)\n",
    "    if random.random() < 0.15:  # 15% dark funnel\n",
    "        self_reported = random.choice(DARK_FUNNEL_SOURCES)\n",
    "    else:\n",
    "        self_reported = random.choice(SELF_REPORTED_TEMPLATES[lead_source])\n",
    "    \n",
    "    created_date = random_date(START_DATE, END_DATE)\n",
    "    \n",
    "    # Lead score (higher for bigger companies, certain sources)\n",
    "    base_score = random.randint(20, 60)\n",
    "    if account['tier'] == 1:\n",
    "        base_score += 25\n",
    "    elif account['tier'] == 2:\n",
    "        base_score += 10\n",
    "    if lead_source in ['referral', 'event']:\n",
    "        base_score += 15\n",
    "    lead_score = min(base_score + random.randint(-10, 10), 100)\n",
    "    \n",
    "    # MQL based on score (higher score = higher MQL probability)\n",
    "    mql_prob = min(0.1 + (lead_score - 40) * 0.015, 0.7)\n",
    "    mql_date = None\n",
    "    sql_date = None\n",
    "    \n",
    "    if random.random() < mql_prob:\n",
    "        mql_date = created_date + timedelta(days=random.randint(1, 30))\n",
    "        if mql_date > END_DATE:\n",
    "            mql_date = None\n",
    "        elif random.random() < 0.20:  # 20% MQL to SQL\n",
    "            sql_date = mql_date + timedelta(days=random.randint(5, 45))\n",
    "            if sql_date > END_DATE:\n",
    "                sql_date = None\n",
    "    \n",
    "    # Company size (with some missing values)\n",
    "    if random.random() < 0.1:\n",
    "        company_size = None\n",
    "    else:\n",
    "        size_map = {1: '1000+', 2: '200-999', 3: '1-199'}\n",
    "        # Sometimes mismatch between lead-provided and account data\n",
    "        if random.random() < 0.85:\n",
    "            company_size = size_map[account['tier']]\n",
    "        else:\n",
    "            company_size = random.choice(['1-199', '200-999', '1000+'])\n",
    "    \n",
    "    lead_id = f'LEAD-{i+1:05d}'\n",
    "    \n",
    "    leads.append({\n",
    "        'lead_id': lead_id,\n",
    "        'created_date': created_date,\n",
    "        'email': generate_email(full_name, account['company_name']),\n",
    "        'company_name': company_name,\n",
    "        'company_size': company_size,\n",
    "        'industry': account['industry'] if random.random() < 0.9 else random.choice(INDUSTRIES),\n",
    "        'lead_source': lead_source,\n",
    "        'self_reported_source': self_reported,\n",
    "        'lead_score': lead_score,\n",
    "        'mql_date': mql_date,\n",
    "        'sql_date': sql_date\n",
    "    })\n",
    "    \n",
    "    contact_to_account.append({\n",
    "        'contact_id': lead_id,\n",
    "        'account_id': account['account_id']\n",
    "    })\n",
    "\n",
    "# Add some duplicate leads (same person, slightly different data)\n",
    "num_duplicates = int(NUM_LEADS * 0.03)  # 3% duplicates\n",
    "for _ in range(num_duplicates):\n",
    "    original = random.choice(leads)\n",
    "    duplicate = original.copy()\n",
    "    duplicate['lead_id'] = f'LEAD-{len(leads)+1:05d}'\n",
    "    duplicate['created_date'] = original['created_date'] + timedelta(days=random.randint(1, 60))\n",
    "    # Slight email variation\n",
    "    if random.random() < 0.5:\n",
    "        duplicate['email'] = duplicate['email'].replace('.', '_', 1)\n",
    "    leads.append(duplicate)\n",
    "    \n",
    "    # Don't add to contact_to_account (simulating the data issue)\n",
    "\n",
    "leads_df = pd.DataFrame(leads)\n",
    "contact_to_account_df = pd.DataFrame(contact_to_account)\n",
    "\n",
    "print(f\"Generated {len(leads_df)} leads (including {num_duplicates} duplicates)\")\n",
    "print(f\"MQL count: {leads_df['mql_date'].notna().sum()}\")\n",
    "print(f\"SQL count: {leads_df['sql_date'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Touches (marketing touchpoints)\n",
    "CHANNELS = ['paid_search', 'linkedin', 'organic', 'email', 'webinar', 'content', 'direct', 'retargeting']\n",
    "CAMPAIGNS = {\n",
    "    'paid_search': ['Brand Search', 'Competitor Search', 'Product Search', 'Generic Search'],\n",
    "    'linkedin': ['LinkedIn Sponsored Content', 'LinkedIn InMail', 'LinkedIn Video', 'LinkedIn Carousel'],\n",
    "    'organic': ['Blog - SEO', 'Documentation', 'Resource Center'],\n",
    "    'email': ['Newsletter', 'Nurture Sequence', 'Product Update', 'Event Invite', 'Re-engagement'],\n",
    "    'webinar': ['Monthly Webinar', 'Product Demo Webinar', 'Industry Expert Webinar'],\n",
    "    'content': ['Ebook Download', 'Whitepaper', 'Case Study', 'ROI Calculator', 'Template Download'],\n",
    "    'direct': ['Direct Visit', 'Typed URL'],\n",
    "    'retargeting': ['Display Retargeting', 'LinkedIn Retargeting', 'Google Retargeting']\n",
    "}\n",
    "CONTENT_ASSETS = {\n",
    "    'ebook': ['Complete Guide to B2B Marketing', 'Data-Driven Marketing Playbook', 'ABM Strategy Guide'],\n",
    "    'whitepaper': ['State of B2B Marketing 2024', 'Attribution Modeling Best Practices', 'Martech Stack Optimization'],\n",
    "    'case_study': ['Case Study: TechCorp', 'Case Study: DataFlow Inc', 'Case Study: CloudFirst'],\n",
    "    'webinar': ['Webinar: Pipeline Acceleration', 'Webinar: Attribution Deep Dive', 'Webinar: ABM Tactics'],\n",
    "    'blog': ['Blog: 10 Tips for B2B Success', 'Blog: Understanding MQL', 'Blog: Sales Marketing Alignment']\n",
    "}\n",
    "\n",
    "touches = []\n",
    "touch_id = 1\n",
    "\n",
    "for _, lead in leads_df.iterrows():\n",
    "    # Determine number of touches based on lead progression\n",
    "    if lead['sql_date'] is not None:\n",
    "        num_touches = random.randint(8, 20)  # Converted leads have more touches\n",
    "    elif lead['mql_date'] is not None:\n",
    "        num_touches = random.randint(4, 12)\n",
    "    else:\n",
    "        num_touches = random.randint(1, 5)\n",
    "    \n",
    "    # First touch is typically the lead source\n",
    "    first_touch_channel = lead['lead_source']\n",
    "    touch_date = lead['created_date']\n",
    "    \n",
    "    for i in range(num_touches):\n",
    "        if i == 0:\n",
    "            channel = first_touch_channel\n",
    "        else:\n",
    "            # Subsequent touches follow realistic patterns\n",
    "            channel_weights = {\n",
    "                'email': 0.30,\n",
    "                'organic': 0.15,\n",
    "                'content': 0.15,\n",
    "                'retargeting': 0.15,\n",
    "                'direct': 0.10,\n",
    "                'linkedin': 0.08,\n",
    "                'webinar': 0.05,\n",
    "                'paid_search': 0.02\n",
    "            }\n",
    "            channel = random.choices(list(channel_weights.keys()), \n",
    "                                    weights=list(channel_weights.values()))[0]\n",
    "        \n",
    "        campaign = random.choice(CAMPAIGNS.get(channel, ['Unknown Campaign']))\n",
    "        \n",
    "        # Content asset (sometimes present)\n",
    "        if channel in ['content', 'organic', 'email', 'webinar']:\n",
    "            asset_type = random.choice(list(CONTENT_ASSETS.keys()))\n",
    "            content_asset = random.choice(CONTENT_ASSETS[asset_type])\n",
    "        else:\n",
    "            content_asset = None\n",
    "        \n",
    "        touches.append({\n",
    "            'touch_id': f'TOUCH-{touch_id:06d}',\n",
    "            'lead_id': lead['lead_id'],\n",
    "            'timestamp': touch_date + timedelta(hours=random.randint(0, 23), minutes=random.randint(0, 59)),\n",
    "            'channel': channel,\n",
    "            'campaign': campaign,\n",
    "            'content_asset': content_asset\n",
    "        })\n",
    "        touch_id += 1\n",
    "        \n",
    "        # Next touch is 1-14 days later\n",
    "        touch_date = touch_date + timedelta(days=random.randint(1, 14))\n",
    "        if touch_date > END_DATE:\n",
    "            break\n",
    "\n",
    "touches_df = pd.DataFrame(touches)\n",
    "print(f\"Generated {len(touches_df)} touchpoints\")\n",
    "print(f\"Average touches per lead: {len(touches_df) / len(leads_df):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Opportunities\n",
    "opportunities = []\n",
    "opp_id = 1\n",
    "\n",
    "# Get SQL leads to create opportunities from\n",
    "sql_leads = leads_df[leads_df['sql_date'].notna()].copy()\n",
    "\n",
    "# 25% of SQLs become opportunities\n",
    "opp_leads = sql_leads.sample(frac=0.25)\n",
    "\n",
    "for _, lead in opp_leads.iterrows():\n",
    "    # Get the account\n",
    "    contact_mapping = contact_to_account_df[contact_to_account_df['contact_id'] == lead['lead_id']]\n",
    "    if len(contact_mapping) == 0:\n",
    "        continue\n",
    "    account_id = contact_mapping.iloc[0]['account_id']\n",
    "    account = accounts_df[accounts_df['account_id'] == account_id].iloc[0]\n",
    "    \n",
    "    created_date = lead['sql_date'] + timedelta(days=random.randint(1, 14))\n",
    "    if created_date > END_DATE:\n",
    "        continue\n",
    "    \n",
    "    # Deal size based on company size\n",
    "    if account['tier'] == 1:\n",
    "        base_amount = random.randint(50000, 250000)\n",
    "    elif account['tier'] == 2:\n",
    "        base_amount = random.randint(20000, 80000)\n",
    "    else:\n",
    "        base_amount = random.randint(5000, 30000)\n",
    "    \n",
    "    # Win rate ~30%\n",
    "    is_won = random.random() < 0.30\n",
    "    \n",
    "    # Sales cycle 60-120 days\n",
    "    cycle_length = random.randint(60, 120)\n",
    "    close_date = created_date + timedelta(days=cycle_length)\n",
    "    \n",
    "    if close_date > END_DATE:\n",
    "        # Still open\n",
    "        stage = random.choice(['Discovery', 'Qualification', 'Proposal', 'Negotiation'])\n",
    "        close_date = None\n",
    "        is_won = None\n",
    "    else:\n",
    "        stage = 'Closed Won' if is_won else 'Closed Lost'\n",
    "    \n",
    "    opportunities.append({\n",
    "        'opp_id': f'OPP-{opp_id:04d}',\n",
    "        'account_id': account_id,\n",
    "        'created_date': created_date,\n",
    "        'close_date': close_date,\n",
    "        'stage': stage,\n",
    "        'amount': base_amount,\n",
    "        'is_won': is_won,\n",
    "        'primary_lead_id': lead['lead_id']\n",
    "    })\n",
    "    opp_id += 1\n",
    "\n",
    "opportunities_df = pd.DataFrame(opportunities)\n",
    "print(f\"Generated {len(opportunities_df)} opportunities\")\n",
    "print(f\"Won: {(opportunities_df['is_won'] == True).sum()}\")\n",
    "print(f\"Lost: {(opportunities_df['is_won'] == False).sum()}\")\n",
    "print(f\"Open: {opportunities_df['is_won'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of generated data\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nAccounts: {len(accounts_df)}\")\n",
    "print(f\"Leads: {len(leads_df)}\")\n",
    "print(f\"Touches: {len(touches_df)}\")\n",
    "print(f\"Opportunities: {len(opportunities_df)}\")\n",
    "print(f\"Contact-Account Mappings: {len(contact_to_account_df)}\")\n",
    "\n",
    "print(\"\\n--- Data Quality Issues (Intentional) ---\")\n",
    "print(f\"Leads with missing company_size: {leads_df['company_size'].isna().sum()}\")\n",
    "print(f\"Leads with missing self_reported_source: {leads_df['self_reported_source'].isna().sum()}\")\n",
    "print(f\"Duplicate leads (approx): {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Data Cleaning\n",
    "\n",
    "Real B2B data is messy. Before any analysis, you need to clean it.\n",
    "\n",
    "**Tasks:**\n",
    "1. Find and remove duplicate leads\n",
    "2. Standardize company names\n",
    "3. Handle missing values\n",
    "4. Normalize lead sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.1: Find duplicate leads\n",
    "# Duplicates might have same email (with slight variations) or same name + company\n",
    "\n",
    "# Create a working copy\n",
    "leads_clean = leads_df.copy()\n",
    "\n",
    "# Normalize email for comparison\n",
    "leads_clean['email_normalized'] = leads_clean['email'].str.lower().str.replace('_', '.')\n",
    "\n",
    "# Find duplicates based on normalized email\n",
    "duplicates = leads_clean[leads_clean.duplicated(subset=['email_normalized'], keep=False)]\n",
    "print(f\"Found {len(duplicates)} records that share an email with another record\")\n",
    "print(f\"\\nExample duplicates:\")\n",
    "duplicates.head(10)[['lead_id', 'email', 'email_normalized', 'created_date', 'company_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.1 Solution: Remove duplicates, keeping the earliest lead\n",
    "leads_clean = leads_clean.sort_values('created_date')\n",
    "leads_clean = leads_clean.drop_duplicates(subset=['email_normalized'], keep='first')\n",
    "print(f\"After deduplication: {len(leads_clean)} leads (removed {len(leads_df) - len(leads_clean)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.2: Standardize company names\n",
    "# Check the variations\n",
    "print(\"Sample of company name variations:\")\n",
    "leads_clean['company_name'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.2 Solution: Standardize company names\n",
    "def standardize_company_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    # Convert to title case\n",
    "    name = name.strip()\n",
    "    # Remove common suffixes for matching\n",
    "    suffixes_to_remove = [', Inc.', ', Inc', ' Inc.', ' Inc', ' LLC', ' Corp', ' Corp.', ' Co.', ' Co']\n",
    "    name_clean = name\n",
    "    for suffix in suffixes_to_remove:\n",
    "        name_clean = name_clean.replace(suffix, '')\n",
    "    # Title case\n",
    "    name_clean = name_clean.title()\n",
    "    return name_clean\n",
    "\n",
    "leads_clean['company_name_std'] = leads_clean['company_name'].apply(standardize_company_name)\n",
    "\n",
    "print(\"Unique company names before standardization:\", leads_clean['company_name'].nunique())\n",
    "print(\"Unique company names after standardization:\", leads_clean['company_name_std'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3: Handle missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(leads_clean.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "# For company_size: we could impute based on account data or leave as 'Unknown'\n",
    "leads_clean['company_size'] = leads_clean['company_size'].fillna('Unknown')\n",
    "\n",
    "# For self_reported_source: leave as NaN - it's meaningful (person didn't answer)\n",
    "print(\"\\nAfter handling missing values:\")\n",
    "print(leads_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.4: Normalize lead sources (already clean in our data, but demonstrate the pattern)\n",
    "print(\"Lead source distribution:\")\n",
    "print(leads_clean['lead_source'].value_counts())\n",
    "\n",
    "# Example: if we had variations like 'LinkedIn', 'linkedin', 'LINKEDIN'\n",
    "leads_clean['lead_source'] = leads_clean['lead_source'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: SQL Queries\n",
    "\n",
    "Real-world marketing data typically lives in databases (data warehouses, CRMs, etc.), so you need SQL skills. We'll use SQLite to run actual SQL queries against our synthetic data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Conversion rates by stage\n",
    "2. Average deal size by segment\n",
    "3. Lead volume by source over time\n",
    "4. Pipeline velocity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SQLite in-memory database\n",
    "import sqlite3\n",
    "\n",
    "# Create connection and load data\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Load all dataframes into SQL tables\n",
    "leads_clean.to_sql('leads', conn, index=False, if_exists='replace')\n",
    "accounts_df.to_sql('accounts', conn, index=False, if_exists='replace')\n",
    "touches_df.to_sql('touches', conn, index=False, if_exists='replace')\n",
    "opportunities_df.to_sql('opportunities', conn, index=False, if_exists='replace')\n",
    "contact_to_account_df.to_sql('contact_to_account', conn, index=False, if_exists='replace')\n",
    "\n",
    "# Helper function to run SQL and display results\n",
    "def run_sql(query, conn=conn):\n",
    "    \"\"\"Execute SQL query and return results as DataFrame.\"\"\"\n",
    "    return pd.read_sql_query(query, conn)\n",
    "\n",
    "print(\"Database tables created:\")\n",
    "print(run_sql(\"SELECT name FROM sqlite_master WHERE type='table'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1: Conversion rates by stage\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_leads,\n",
    "    SUM(CASE WHEN mql_date IS NOT NULL THEN 1 ELSE 0 END) as mqls,\n",
    "    SUM(CASE WHEN sql_date IS NOT NULL THEN 1 ELSE 0 END) as sqls,\n",
    "    ROUND(SUM(CASE WHEN mql_date IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as lead_to_mql_pct,\n",
    "    ROUND(SUM(CASE WHEN sql_date IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / \n",
    "          NULLIF(SUM(CASE WHEN mql_date IS NOT NULL THEN 1 ELSE 0 END), 0), 2) as mql_to_sql_pct\n",
    "FROM leads;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Funnel Conversion Rates (SQL)\")\n",
    "print(\"=\" * 50)\n",
    "result = run_sql(query)\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "# Add opportunity conversions\n",
    "opp_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_opportunities,\n",
    "    SUM(CASE WHEN is_won = 1 THEN 1 ELSE 0 END) as closed_won,\n",
    "    ROUND(SUM(CASE WHEN is_won = 1 THEN 1 ELSE 0 END) * 100.0 / \n",
    "          NULLIF(SUM(CASE WHEN is_won IS NOT NULL THEN 1 ELSE 0 END), 0), 2) as win_rate_pct\n",
    "FROM opportunities;\n",
    "\"\"\"\n",
    "print(\"\\nOpportunity Win Rate:\")\n",
    "print(run_sql(opp_query).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.2: Average deal size by segment\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    a.tier,\n",
    "    COUNT(o.opp_id) as num_opps,\n",
    "    ROUND(AVG(o.amount), 0) as avg_deal_size,\n",
    "    SUM(o.amount) as total_pipeline\n",
    "FROM opportunities o\n",
    "JOIN accounts a ON o.account_id = a.account_id\n",
    "GROUP BY a.tier\n",
    "ORDER BY a.tier;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Deal Size by Account Tier (SQL)\")\n",
    "print(\"=\" * 50)\n",
    "print(run_sql(query).to_string(index=False))\n",
    "\n",
    "# By industry\n",
    "query_industry = \"\"\"\n",
    "SELECT \n",
    "    a.industry,\n",
    "    COUNT(o.opp_id) as num_opps,\n",
    "    ROUND(AVG(o.amount), 0) as avg_deal_size,\n",
    "    SUM(o.amount) as total_pipeline\n",
    "FROM opportunities o\n",
    "JOIN accounts a ON o.account_id = a.account_id\n",
    "GROUP BY a.industry\n",
    "ORDER BY avg_deal_size DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nDeal Size by Industry (SQL)\")\n",
    "print(\"=\" * 50)\n",
    "print(run_sql(query_industry).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.3: Lead volume by source over time\n",
    "# Note: SQLite uses strftime instead of DATE_TRUNC\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    strftime('%Y-%m', created_date) as month,\n",
    "    lead_source,\n",
    "    COUNT(*) as lead_count\n",
    "FROM leads\n",
    "GROUP BY strftime('%Y-%m', created_date), lead_source\n",
    "ORDER BY month, lead_source;\n",
    "\"\"\"\n",
    "\n",
    "result = run_sql(query)\n",
    "\n",
    "# Pivot for better readability\n",
    "pivot = result.pivot(index='month', columns='lead_source', values='lead_count').fillna(0).astype(int)\n",
    "print(\"Monthly Lead Volume by Source (SQL)\")\n",
    "print(\"=\" * 60)\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.4: Pipeline velocity calculation\n",
    "# Velocity = (Number of Opps × Win Rate × Avg Deal Size) / Sales Cycle Length\n",
    "\n",
    "query = \"\"\"\n",
    "WITH closed_deals AS (\n",
    "    SELECT \n",
    "        COUNT(*) as total_closed,\n",
    "        SUM(CASE WHEN is_won = 1 THEN 1 ELSE 0 END) as won_opps,\n",
    "        AVG(amount) as avg_deal_size,\n",
    "        AVG(julianday(close_date) - julianday(created_date)) as avg_cycle_days\n",
    "    FROM opportunities\n",
    "    WHERE close_date IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    total_closed,\n",
    "    won_opps,\n",
    "    ROUND(won_opps * 100.0 / total_closed, 1) as win_rate_pct,\n",
    "    ROUND(avg_deal_size, 0) as avg_deal_size,\n",
    "    ROUND(avg_cycle_days, 0) as avg_cycle_days,\n",
    "    ROUND((won_opps * avg_deal_size) / avg_cycle_days, 0) as daily_velocity\n",
    "FROM closed_deals;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pipeline Velocity Analysis (SQL)\")\n",
    "print(\"=\" * 50)\n",
    "result = run_sql(query)\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "# Calculate monthly velocity\n",
    "daily_vel = result['daily_velocity'].values[0]\n",
    "print(f\"\\nMonthly Revenue Velocity: ${daily_vel * 30:,.0f}/month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store key metrics as variables for later exercises\n",
    "# (These are used in visualization exercises later)\n",
    "\n",
    "total_leads = len(leads_clean)\n",
    "total_mqls = leads_clean['mql_date'].notna().sum()\n",
    "total_sqls = leads_clean['sql_date'].notna().sum()\n",
    "total_opps = len(opportunities_df)\n",
    "won_opps = (opportunities_df['is_won'] == True).sum()\n",
    "\n",
    "print(\"Summary metrics stored for later exercises:\")\n",
    "print(f\"  total_leads: {total_leads:,}\")\n",
    "print(f\"  total_mqls: {total_mqls:,}\")\n",
    "print(f\"  total_sqls: {total_sqls:,}\")\n",
    "print(f\"  total_opps: {total_opps:,}\")\n",
    "print(f\"  won_opps: {won_opps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Conversion Rate Analysis\n",
    "\n",
    "Dig deeper into conversion rates by segment to identify best and worst performers.\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate stage-by-stage conversion rates\n",
    "2. Segment by lead source, company size, industry\n",
    "3. Identify best/worst performing segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1: Stage-by-stage conversion rates by lead source\n",
    "\n",
    "def calculate_conversion_rates(df, group_col):\n",
    "    \"\"\"Calculate funnel conversion rates grouped by a column.\"\"\"\n",
    "    grouped = df.groupby(group_col).agg({\n",
    "        'lead_id': 'count',\n",
    "        'mql_date': lambda x: x.notna().sum(),\n",
    "        'sql_date': lambda x: x.notna().sum()\n",
    "    })\n",
    "    grouped.columns = ['leads', 'mqls', 'sqls']\n",
    "    \n",
    "    grouped['lead_to_mql'] = (grouped['mqls'] / grouped['leads'] * 100).round(1)\n",
    "    grouped['mql_to_sql'] = (grouped['sqls'] / grouped['mqls'] * 100).round(1)\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "# By lead source\n",
    "source_conversion = calculate_conversion_rates(leads_clean, 'lead_source')\n",
    "print(\"Conversion Rates by Lead Source\")\n",
    "print(source_conversion.sort_values('lead_to_mql', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2: By company size\n",
    "size_conversion = calculate_conversion_rates(leads_clean, 'company_size')\n",
    "print(\"Conversion Rates by Company Size\")\n",
    "print(size_conversion.sort_values('lead_to_mql', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3: By industry\n",
    "industry_conversion = calculate_conversion_rates(leads_clean, 'industry')\n",
    "print(\"Conversion Rates by Industry\")\n",
    "print(industry_conversion.sort_values('lead_to_mql', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4: Identify best and worst performing segments\n",
    "# Combine source + company size for deeper analysis\n",
    "\n",
    "leads_clean['segment'] = leads_clean['lead_source'] + ' | ' + leads_clean['company_size']\n",
    "\n",
    "segment_conversion = calculate_conversion_rates(leads_clean, 'segment')\n",
    "# Filter for segments with enough volume (at least 50 leads)\n",
    "segment_conversion = segment_conversion[segment_conversion['leads'] >= 50]\n",
    "\n",
    "print(\"TOP 5 SEGMENTS (by Lead→MQL conversion)\")\n",
    "print(segment_conversion.sort_values('lead_to_mql', ascending=False).head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"\\nBOTTOM 5 SEGMENTS (by Lead→MQL conversion)\")\n",
    "print(segment_conversion.sort_values('lead_to_mql', ascending=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: Build Attribution Models\n",
    "\n",
    "Implement the core attribution models from scratch.\n",
    "\n",
    "**Tasks:**\n",
    "1. First-touch attribution\n",
    "2. Last-touch attribution\n",
    "3. Linear attribution\n",
    "4. Time-decay attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get touches for leads that became opportunities (where we have pipeline to attribute)\n",
    "opp_lead_ids = opportunities_df['primary_lead_id'].tolist()\n",
    "opp_touches = touches_df[touches_df['lead_id'].isin(opp_lead_ids)].copy()\n",
    "\n",
    "# Merge with opportunity amount for attribution\n",
    "opp_touches = opp_touches.merge(\n",
    "    opportunities_df[['primary_lead_id', 'amount', 'opp_id']], \n",
    "    left_on='lead_id', \n",
    "    right_on='primary_lead_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Touchpoints for opportunities: {len(opp_touches)}\")\n",
    "print(f\"Unique opportunities: {opp_touches['opp_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1: First-Touch Attribution\n",
    "# All credit goes to the first touchpoint\n",
    "\n",
    "def first_touch_attribution(touches_df):\n",
    "    \"\"\"Attribute all pipeline credit to the first touch.\"\"\"\n",
    "    # Sort by timestamp and get first touch per opportunity\n",
    "    first_touches = (touches_df\n",
    "                     .sort_values('timestamp')\n",
    "                     .groupby('opp_id')\n",
    "                     .first()\n",
    "                     .reset_index())\n",
    "    \n",
    "    # Sum by channel\n",
    "    attribution = (first_touches\n",
    "                   .groupby('channel')\n",
    "                   .agg({'amount': 'sum', 'opp_id': 'count'})\n",
    "                   .rename(columns={'amount': 'pipeline', 'opp_id': 'opps'}))\n",
    "    \n",
    "    return attribution.sort_values('pipeline', ascending=False)\n",
    "\n",
    "first_touch = first_touch_attribution(opp_touches)\n",
    "print(\"First-Touch Attribution\")\n",
    "print(first_touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2: Last-Touch Attribution\n",
    "# All credit goes to the last touchpoint before opportunity creation\n",
    "\n",
    "def last_touch_attribution(touches_df):\n",
    "    \"\"\"Attribute all pipeline credit to the last touch.\"\"\"\n",
    "    # Sort by timestamp and get last touch per opportunity\n",
    "    last_touches = (touches_df\n",
    "                    .sort_values('timestamp')\n",
    "                    .groupby('opp_id')\n",
    "                    .last()\n",
    "                    .reset_index())\n",
    "    \n",
    "    # Sum by channel\n",
    "    attribution = (last_touches\n",
    "                   .groupby('channel')\n",
    "                   .agg({'amount': 'sum', 'opp_id': 'count'})\n",
    "                   .rename(columns={'amount': 'pipeline', 'opp_id': 'opps'}))\n",
    "    \n",
    "    return attribution.sort_values('pipeline', ascending=False)\n",
    "\n",
    "last_touch = last_touch_attribution(opp_touches)\n",
    "print(\"Last-Touch Attribution\")\n",
    "print(last_touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3: Linear Attribution\n",
    "# Credit split equally across all touchpoints\n",
    "\n",
    "def linear_attribution(touches_df):\n",
    "    \"\"\"Split pipeline credit equally across all touches.\"\"\"\n",
    "    # Count touches per opportunity\n",
    "    touch_counts = touches_df.groupby('opp_id').size().reset_index(name='touch_count')\n",
    "    \n",
    "    # Merge back to get credit per touch\n",
    "    touches_with_counts = touches_df.merge(touch_counts, on='opp_id')\n",
    "    touches_with_counts['credit'] = touches_with_counts['amount'] / touches_with_counts['touch_count']\n",
    "    \n",
    "    # Sum by channel\n",
    "    attribution = (touches_with_counts\n",
    "                   .groupby('channel')\n",
    "                   .agg({'credit': 'sum', 'touch_id': 'count'})\n",
    "                   .rename(columns={'credit': 'pipeline', 'touch_id': 'touches'}))\n",
    "    \n",
    "    return attribution.sort_values('pipeline', ascending=False)\n",
    "\n",
    "linear = linear_attribution(opp_touches)\n",
    "print(\"Linear Attribution\")\n",
    "print(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4: Time-Decay Attribution\n",
    "# More recent touches get more credit (exponential decay)\n",
    "\n",
    "def time_decay_attribution(touches_df, half_life_days=7):\n",
    "    \"\"\"Attribute more credit to recent touches using exponential decay.\"\"\"\n",
    "    df = touches_df.copy()\n",
    "    \n",
    "    # Get the last touch timestamp per opportunity\n",
    "    last_touch_time = df.groupby('opp_id')['timestamp'].max().reset_index()\n",
    "    last_touch_time.columns = ['opp_id', 'last_touch']\n",
    "    \n",
    "    df = df.merge(last_touch_time, on='opp_id')\n",
    "    \n",
    "    # Calculate days before last touch\n",
    "    df['days_before_last'] = (pd.to_datetime(df['last_touch']) - \n",
    "                               pd.to_datetime(df['timestamp'])).dt.days\n",
    "    \n",
    "    # Apply exponential decay: weight = 2^(-days/half_life)\n",
    "    df['weight'] = 2 ** (-df['days_before_last'] / half_life_days)\n",
    "    \n",
    "    # Normalize weights within each opportunity\n",
    "    weight_sums = df.groupby('opp_id')['weight'].sum().reset_index(name='weight_sum')\n",
    "    df = df.merge(weight_sums, on='opp_id')\n",
    "    df['normalized_weight'] = df['weight'] / df['weight_sum']\n",
    "    \n",
    "    # Calculate credit\n",
    "    df['credit'] = df['amount'] * df['normalized_weight']\n",
    "    \n",
    "    # Sum by channel\n",
    "    attribution = (df\n",
    "                   .groupby('channel')\n",
    "                   .agg({'credit': 'sum', 'touch_id': 'count'})\n",
    "                   .rename(columns={'credit': 'pipeline', 'touch_id': 'touches'}))\n",
    "    \n",
    "    return attribution.sort_values('pipeline', ascending=False)\n",
    "\n",
    "time_decay = time_decay_attribution(opp_touches, half_life_days=7)\n",
    "print(\"Time-Decay Attribution (7-day half-life)\")\n",
    "print(time_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5: Attribution Comparison Analysis\n",
    "\n",
    "Compare how different attribution models value each channel.\n",
    "\n",
    "**Tasks:**\n",
    "1. Compare channel rankings across models\n",
    "2. Visualize differences\n",
    "3. Write up implications for budget allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1: Compare channel rankings\n",
    "\n",
    "# Combine all models\n",
    "comparison = pd.DataFrame({\n",
    "    'first_touch': first_touch['pipeline'],\n",
    "    'last_touch': last_touch['pipeline'],\n",
    "    'linear': linear['pipeline'],\n",
    "    'time_decay': time_decay['pipeline']\n",
    "}).fillna(0)\n",
    "\n",
    "# Add percentage of total\n",
    "for col in comparison.columns:\n",
    "    comparison[f'{col}_pct'] = (comparison[col] / comparison[col].sum() * 100).round(1)\n",
    "\n",
    "print(\"Attribution Comparison (Pipeline $)\")\n",
    "print(comparison[['first_touch', 'last_touch', 'linear', 'time_decay']].round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2: Visualize differences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(comparison))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax.bar([i - 1.5*width for i in x], comparison['first_touch']/1000, width, label='First Touch', color='#2563eb')\n",
    "bars2 = ax.bar([i - 0.5*width for i in x], comparison['last_touch']/1000, width, label='Last Touch', color='#7c3aed')\n",
    "bars3 = ax.bar([i + 0.5*width for i in x], comparison['linear']/1000, width, label='Linear', color='#059669')\n",
    "bars4 = ax.bar([i + 1.5*width for i in x], comparison['time_decay']/1000, width, label='Time Decay', color='#dc2626')\n",
    "\n",
    "ax.set_ylabel('Pipeline ($K)')\n",
    "ax.set_title('Attribution Model Comparison by Channel')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attribution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.3: Identify winners and losers by model\n",
    "\n",
    "# Calculate rank for each channel under each model\n",
    "rankings = comparison[['first_touch', 'last_touch', 'linear', 'time_decay']].rank(ascending=False)\n",
    "rankings.columns = ['ft_rank', 'lt_rank', 'lin_rank', 'td_rank']\n",
    "\n",
    "# Find channels with biggest rank changes\n",
    "rankings['rank_variance'] = rankings.var(axis=1)\n",
    "rankings = rankings.sort_values('rank_variance', ascending=False)\n",
    "\n",
    "print(\"Channel Rankings by Attribution Model\")\n",
    "print(\"(1 = highest pipeline credit)\")\n",
    "print(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribution Implications for Budget Allocation\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **First-touch favors awareness channels** (paid search, LinkedIn, content syndication) that bring people in initially. If you use first-touch, you'll invest more in top-of-funnel.\n",
    "\n",
    "2. **Last-touch favors conversion channels** (direct, retargeting, email) that catch people right before they convert. This often overstates the value of branded search and remarketing.\n",
    "\n",
    "3. **Linear gives a balanced view** but can undervalue both introduction and closing channels by spreading credit too thin.\n",
    "\n",
    "4. **Time-decay is often most realistic for B2B** because it recognizes that recent touches matter more, while still giving credit to earlier touches.\n",
    "\n",
    "**Recommendation:** Don't rely on a single model. Present multiple models to stakeholders and explain the trade-offs. Time-decay is often a good default for B2B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6: Self-Reported vs Tracked Attribution\n",
    "\n",
    "Compare what people say (\"how did you hear about us?\") vs what tracking shows.\n",
    "\n",
    "**Tasks:**\n",
    "1. Compare self-reported source to first-touch tracked source\n",
    "2. Calculate agreement rate\n",
    "3. Identify dark funnel indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.1: Map self-reported sources to standard categories\n",
    "\n",
    "def categorize_self_reported(text):\n",
    "    \"\"\"Map free-text self-reported sources to standard categories.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'not_provided'\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    if any(x in text for x in ['google', 'search', 'seo', 'found your website']):\n",
    "        return 'search'\n",
    "    elif any(x in text for x in ['linkedin', 'li']):\n",
    "        return 'linkedin'\n",
    "    elif any(x in text for x in ['referral', 'colleague', 'friend', 'recommend', 'word of mouth', 'boss', 'peer']):\n",
    "        return 'referral'\n",
    "    elif any(x in text for x in ['webinar', 'conference', 'event', 'trade show', 'dreamforce', 'saastr']):\n",
    "        return 'event'\n",
    "    elif any(x in text for x in ['blog', 'article', 'content', 'whitepaper', 'ebook', 'report', 'download']):\n",
    "        return 'content'\n",
    "    elif any(x in text for x in ['podcast', 'youtube', 'twitter', 'slack', 'newsletter', 'following']):\n",
    "        return 'dark_funnel'  # Channels we can't track well\n",
    "    elif any(x in text for x in [\"can't remember\", 'not sure']):\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "leads_clean['self_reported_category'] = leads_clean['self_reported_source'].apply(categorize_self_reported)\n",
    "\n",
    "print(\"Self-Reported Source Categories\")\n",
    "print(leads_clean['self_reported_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.2: Get first-touch for each lead\n",
    "\n",
    "first_touches_all = (touches_df\n",
    "                     .sort_values('timestamp')\n",
    "                     .groupby('lead_id')\n",
    "                     .first()\n",
    "                     .reset_index()[['lead_id', 'channel']])\n",
    "\n",
    "first_touches_all.columns = ['lead_id', 'first_touch_channel']\n",
    "\n",
    "# Merge with leads\n",
    "leads_with_ft = leads_clean.merge(first_touches_all, on='lead_id', how='left')\n",
    "\n",
    "# Map first-touch to comparable categories\n",
    "def map_tracked_to_category(channel):\n",
    "    if pd.isna(channel):\n",
    "        return 'unknown'\n",
    "    \n",
    "    mapping = {\n",
    "        'paid_search': 'search',\n",
    "        'organic': 'search',\n",
    "        'linkedin': 'linkedin',\n",
    "        'referral': 'referral',\n",
    "        'event': 'event',\n",
    "        'webinar': 'event',\n",
    "        'content': 'content',\n",
    "        'content_syndication': 'content',\n",
    "        'email': 'email',\n",
    "        'direct': 'direct',\n",
    "        'retargeting': 'retargeting'\n",
    "    }\n",
    "    return mapping.get(channel, 'other')\n",
    "\n",
    "leads_with_ft['first_touch_category'] = leads_with_ft['first_touch_channel'].apply(map_tracked_to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.3: Calculate agreement rate\n",
    "\n",
    "# Exclude cases where self-reported wasn't provided\n",
    "comparable = leads_with_ft[\n",
    "    (leads_with_ft['self_reported_category'] != 'not_provided') &\n",
    "    (leads_with_ft['self_reported_category'] != 'unknown')\n",
    "].copy()\n",
    "\n",
    "comparable['match'] = comparable['self_reported_category'] == comparable['first_touch_category']\n",
    "\n",
    "agreement_rate = comparable['match'].mean()\n",
    "print(f\"Agreement Rate: {agreement_rate:.1%}\")\n",
    "print(f\"(Self-reported matches first-touch tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.4: Cross-tabulation to see patterns\n",
    "\n",
    "crosstab = pd.crosstab(\n",
    "    comparable['first_touch_category'], \n",
    "    comparable['self_reported_category'],\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(\"Cross-tab: First Touch (rows) vs Self-Reported (columns)\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.5: Dark funnel analysis\n",
    "\n",
    "dark_funnel_leads = leads_with_ft[leads_with_ft['self_reported_category'] == 'dark_funnel']\n",
    "\n",
    "print(f\"Dark Funnel Leads: {len(dark_funnel_leads)} ({100*len(dark_funnel_leads)/len(leads_with_ft):.1f}% of total)\")\n",
    "print(\"\\nThese leads self-reported sources we can't track (podcasts, YouTube, community, etc.)\")\n",
    "print(\"\\nWhat tracking THOUGHT was their first touch:\")\n",
    "print(dark_funnel_leads['first_touch_channel'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dark Funnel Insights\n",
    "\n",
    "The dark funnel represents touchpoints we can't track:\n",
    "- Word of mouth\n",
    "- Podcasts\n",
    "- Private communities (Slack, Discord)\n",
    "- YouTube\n",
    "- Social (organic, not ads)\n",
    "\n",
    "**Key insight:** When someone says they heard about us on a podcast but tracking shows \"organic search\" as first touch, the tracking is wrong. They searched for us *because* of the podcast.\n",
    "\n",
    "**Recommendations:**\n",
    "1. Always collect \"how did you hear about us?\" - it catches what tracking misses\n",
    "2. Don't treat first-touch tracking as truth - it's often the first *trackable* touch\n",
    "3. Invest in dark funnel channels (podcasts, community) even if they're hard to attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 7: Cohort Analysis\n",
    "\n",
    "Track how cohorts of leads progress through the funnel over time.\n",
    "\n",
    "**Tasks:**\n",
    "1. Define cohorts by signup month\n",
    "2. Track conversion rates over time\n",
    "3. Create cohort heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.1: Define cohorts\n",
    "\n",
    "leads_clean['cohort'] = pd.to_datetime(leads_clean['created_date']).dt.to_period('M')\n",
    "\n",
    "# Calculate days to MQL for each lead\n",
    "leads_clean['days_to_mql'] = (\n",
    "    pd.to_datetime(leads_clean['mql_date']) - \n",
    "    pd.to_datetime(leads_clean['created_date'])\n",
    ").dt.days\n",
    "\n",
    "print(\"Leads per cohort:\")\n",
    "print(leads_clean['cohort'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.2: Track conversion at different time intervals\n",
    "\n",
    "def cohort_conversion_by_days(leads_df, max_days_list=[7, 14, 30, 60, 90]):\n",
    "    \"\"\"Calculate MQL conversion rate at different days after signup.\"\"\"\n",
    "    cohorts = leads_df.groupby('cohort').size().to_frame('total_leads')\n",
    "    \n",
    "    for days in max_days_list:\n",
    "        # Count leads that converted within X days\n",
    "        converted = leads_df[\n",
    "            (leads_df['days_to_mql'].notna()) & \n",
    "            (leads_df['days_to_mql'] <= days)\n",
    "        ].groupby('cohort').size()\n",
    "        \n",
    "        cohorts[f'mql_{days}d'] = converted\n",
    "        cohorts[f'mql_{days}d_pct'] = (cohorts[f'mql_{days}d'] / cohorts['total_leads'] * 100).round(1)\n",
    "    \n",
    "    return cohorts.fillna(0)\n",
    "\n",
    "cohort_analysis = cohort_conversion_by_days(leads_clean)\n",
    "print(\"Cohort Analysis: MQL Conversion by Days After Signup\")\n",
    "print(cohort_analysis[['total_leads', 'mql_7d_pct', 'mql_14d_pct', 'mql_30d_pct', 'mql_60d_pct', 'mql_90d_pct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.3: Create cohort heatmap\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = cohort_analysis[['mql_7d_pct', 'mql_14d_pct', 'mql_30d_pct', 'mql_60d_pct', 'mql_90d_pct']].copy()\n",
    "heatmap_data.columns = ['7 days', '14 days', '30 days', '60 days', '90 days']\n",
    "\n",
    "# Only show cohorts with enough time to mature (exclude last 3 months for 90-day metric)\n",
    "heatmap_data = heatmap_data.iloc[:-3]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(heatmap_data.values, cmap='YlGn', aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(len(heatmap_data.columns)))\n",
    "ax.set_xticklabels(heatmap_data.columns)\n",
    "ax.set_yticks(range(len(heatmap_data.index)))\n",
    "ax.set_yticklabels([str(p) for p in heatmap_data.index])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(heatmap_data.index)):\n",
    "    for j in range(len(heatmap_data.columns)):\n",
    "        text = ax.text(j, i, f'{heatmap_data.iloc[i, j]:.1f}%',\n",
    "                       ha='center', va='center', color='black', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Days Since Signup')\n",
    "ax.set_ylabel('Cohort (Month)')\n",
    "ax.set_title('MQL Conversion Rate by Cohort and Time')\n",
    "\n",
    "plt.colorbar(im, label='Conversion Rate %')\n",
    "plt.tight_layout()\n",
    "plt.savefig('cohort_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 8: Pipeline Contribution Report\n",
    "\n",
    "Build a marketing pipeline contribution report.\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate sourced pipeline by channel (first-touch)\n",
    "2. Calculate influenced pipeline by channel (any-touch)\n",
    "3. Build summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.1: Sourced pipeline (first-touch)\n",
    "# Pipeline where the channel was the FIRST touchpoint\n",
    "\n",
    "sourced = first_touch_attribution(opp_touches)\n",
    "sourced.columns = ['sourced_pipeline', 'sourced_opps']\n",
    "\n",
    "print(\"Sourced Pipeline by Channel\")\n",
    "print(sourced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.2: Influenced pipeline (any-touch)\n",
    "# Pipeline where the channel touched the deal at ANY point\n",
    "\n",
    "def influenced_pipeline(touches_df):\n",
    "    \"\"\"Calculate pipeline where channel had any touch.\"\"\"\n",
    "    # Get unique channel-opportunity combinations\n",
    "    channel_opps = touches_df[['channel', 'opp_id', 'amount']].drop_duplicates(subset=['channel', 'opp_id'])\n",
    "    \n",
    "    # Sum by channel (each opp counted once per channel)\n",
    "    influenced = channel_opps.groupby('channel').agg({\n",
    "        'amount': 'sum',\n",
    "        'opp_id': 'count'\n",
    "    })\n",
    "    influenced.columns = ['influenced_pipeline', 'influenced_opps']\n",
    "    \n",
    "    return influenced.sort_values('influenced_pipeline', ascending=False)\n",
    "\n",
    "influenced = influenced_pipeline(opp_touches)\n",
    "print(\"Influenced Pipeline by Channel\")\n",
    "print(influenced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.3: Combined report\n",
    "\n",
    "# Merge sourced and influenced\n",
    "pipeline_report = sourced.join(influenced, how='outer').fillna(0)\n",
    "\n",
    "# Add totals\n",
    "total_pipeline = opportunities_df['amount'].sum()\n",
    "pipeline_report['sourced_pct'] = (pipeline_report['sourced_pipeline'] / total_pipeline * 100).round(1)\n",
    "pipeline_report['influenced_pct'] = (pipeline_report['influenced_pipeline'] / total_pipeline * 100).round(1)\n",
    "\n",
    "# Influenced / Sourced ratio (how much leverage does a channel have?)\n",
    "pipeline_report['leverage_ratio'] = (pipeline_report['influenced_pipeline'] / \n",
    "                                      pipeline_report['sourced_pipeline'].replace(0, 1)).round(2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MARKETING PIPELINE CONTRIBUTION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Pipeline: ${total_pipeline:,.0f}\")\n",
    "print(f\"Total Opportunities: {len(opportunities_df)}\")\n",
    "print(\"\\n\")\n",
    "print(pipeline_report.sort_values('sourced_pipeline', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sourced vs influenced\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(pipeline_report))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], pipeline_report['sourced_pipeline']/1000, \n",
    "               width, label='Sourced', color='#2563eb')\n",
    "bars2 = ax.bar([i + width/2 for i in x], pipeline_report['influenced_pipeline']/1000, \n",
    "               width, label='Influenced', color='#7c3aed', alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('Pipeline ($K)')\n",
    "ax.set_title('Sourced vs Influenced Pipeline by Channel')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(pipeline_report.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pipeline_contribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 9: Design a Holdout Test\n",
    "\n",
    "Conceptual exercise: design an incrementality test for LinkedIn ads.\n",
    "\n",
    "**Scenario:** Marketing wants to know if LinkedIn ads actually drive incremental pipeline, or if those leads would have converted anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Test Design\n",
    "\n",
    "**Objective:** Measure the true incremental impact of LinkedIn advertising on pipeline generation.\n",
    "\n",
    "**Approach: Geographic Holdout**\n",
    "\n",
    "1. **Select test and control regions:**\n",
    "   - Divide your target markets into similar groups (by size, industry mix, historical performance)\n",
    "   - Example: Hold out 20% of metro areas from LinkedIn ads\n",
    "\n",
    "2. **Duration:** Run for 90+ days (one full sales cycle)\n",
    "\n",
    "3. **Measurement:**\n",
    "   - Track pipeline generated per lead in test vs control regions\n",
    "   - Control for other marketing activities\n",
    "\n",
    "4. **Calculate incrementality:**\n",
    "   - Incremental pipeline = (Test region pipeline/lead - Control region pipeline/lead) × Test leads\n",
    "   - Lift = (Test - Control) / Control\n",
    "\n",
    "**Statistical Requirements:**\n",
    "- Minimum sample size per group to detect 20% lift with 80% power\n",
    "- Account for multiple comparisons if testing multiple channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: Code scaffold for analyzing holdout test results\n",
    "\n",
    "def analyze_holdout_test(test_data, control_data, metric='pipeline_per_lead'):\n",
    "    \"\"\"\n",
    "    Analyze results from a holdout/incrementality test.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_data: DataFrame with leads/pipeline in test group (exposed to ads)\n",
    "    - control_data: DataFrame with leads/pipeline in control group (no ads)\n",
    "    - metric: What to measure\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with test results and statistical significance\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_metric = test_data[metric].mean()\n",
    "    control_metric = control_data[metric].mean()\n",
    "    \n",
    "    # Lift\n",
    "    lift = (test_metric - control_metric) / control_metric if control_metric > 0 else 0\n",
    "    \n",
    "    # Statistical significance (t-test)\n",
    "    t_stat, p_value = stats.ttest_ind(test_data[metric], control_data[metric])\n",
    "    \n",
    "    # Incremental value\n",
    "    incremental_per_unit = test_metric - control_metric\n",
    "    total_incremental = incremental_per_unit * len(test_data)\n",
    "    \n",
    "    results = {\n",
    "        'test_mean': test_metric,\n",
    "        'control_mean': control_metric,\n",
    "        'lift': lift,\n",
    "        'p_value': p_value,\n",
    "        'is_significant': p_value < 0.05,\n",
    "        'incremental_per_unit': incremental_per_unit,\n",
    "        'total_incremental_value': total_incremental\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (with synthetic data):\n",
    "np.random.seed(42)\n",
    "test_group = pd.DataFrame({'pipeline_per_lead': np.random.normal(5000, 2000, 500)})\n",
    "control_group = pd.DataFrame({'pipeline_per_lead': np.random.normal(4200, 2000, 500)})\n",
    "\n",
    "results = analyze_holdout_test(test_group, control_group)\n",
    "\n",
    "print(\"Holdout Test Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Test Group Mean: ${results['test_mean']:,.0f}\")\n",
    "print(f\"Control Group Mean: ${results['control_mean']:,.0f}\")\n",
    "print(f\"Lift: {results['lift']:.1%}\")\n",
    "print(f\"P-value: {results['p_value']:.4f}\")\n",
    "print(f\"Statistically Significant: {results['is_significant']}\")\n",
    "print(f\"\\nIncremental Pipeline per Lead: ${results['incremental_per_unit']:,.0f}\")\n",
    "print(f\"Total Incremental Value: ${results['total_incremental_value']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 10: Visualization Exercises\n",
    "\n",
    "Create the key visualizations every marketing analyst needs.\n",
    "\n",
    "**Tasks:**\n",
    "1. Funnel chart\n",
    "2. Attribution model comparison (done in Ex 5)\n",
    "3. Cohort heatmap (done in Ex 7)\n",
    "4. Pipeline waterfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10.1: Funnel Chart\n",
    "\n",
    "funnel_stages = ['Leads', 'MQLs', 'SQLs', 'Opportunities', 'Closed Won']\n",
    "funnel_values = [total_leads, total_mqls, total_sqls, total_opps, won_opps]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create horizontal bar chart (simulating funnel)\n",
    "colors = ['#3b82f6', '#6366f1', '#8b5cf6', '#a855f7', '#22c55e']\n",
    "y_positions = range(len(funnel_stages))\n",
    "\n",
    "# Normalize widths to create funnel effect\n",
    "max_val = max(funnel_values)\n",
    "widths = [v for v in funnel_values]\n",
    "\n",
    "bars = ax.barh(y_positions, widths, color=colors, height=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, (stage, value) in enumerate(zip(funnel_stages, funnel_values)):\n",
    "    # Conversion rate from previous stage\n",
    "    if i > 0:\n",
    "        conv_rate = value / funnel_values[i-1] * 100\n",
    "        label = f\"{stage}: {value:,} ({conv_rate:.1f}%)\"\n",
    "    else:\n",
    "        label = f\"{stage}: {value:,}\"\n",
    "    ax.text(value + max_val*0.02, i, label, va='center', fontsize=11)\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.set_xlim(0, max_val * 1.3)\n",
    "ax.set_title('Marketing Funnel', fontsize=14, fontweight='bold')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('funnel_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10.4: Pipeline Waterfall Chart\n",
    "\n",
    "# Show pipeline changes: Starting → New → Moved Up → Moved Down → Lost → Ending\n",
    "# Simplified version showing sources of pipeline\n",
    "\n",
    "pipeline_sources = pipeline_report.sort_values('sourced_pipeline', ascending=False).head(6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "channels = pipeline_sources.index.tolist()\n",
    "values = pipeline_sources['sourced_pipeline'].values / 1000\n",
    "\n",
    "# Waterfall logic\n",
    "cumulative = np.zeros(len(values) + 1)\n",
    "cumulative[1:] = np.cumsum(values)\n",
    "\n",
    "# Starting positions for bars\n",
    "starts = cumulative[:-1]\n",
    "\n",
    "colors = ['#3b82f6', '#6366f1', '#8b5cf6', '#a855f7', '#ec4899', '#f97316']\n",
    "\n",
    "bars = ax.bar(channels, values, bottom=starts, color=colors[:len(channels)], edgecolor='white', linewidth=2)\n",
    "\n",
    "# Add total bar\n",
    "ax.bar('TOTAL', cumulative[-1], color='#22c55e', edgecolor='white', linewidth=2)\n",
    "\n",
    "# Add connecting lines\n",
    "for i in range(len(channels)):\n",
    "    ax.plot([i + 0.4, i + 0.6], [cumulative[i+1], cumulative[i+1]], 'k-', linewidth=1)\n",
    "\n",
    "# Labels\n",
    "for i, (channel, value) in enumerate(zip(channels, values)):\n",
    "    ax.text(i, starts[i] + value/2, f'${value:.0f}K', ha='center', va='center', \n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "ax.text(len(channels), cumulative[-1]/2, f'${cumulative[-1]:.0f}K', ha='center', va='center',\n",
    "        fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "ax.set_ylabel('Pipeline ($K)')\n",
    "ax.set_title('Pipeline Waterfall by Source Channel', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(channels + ['TOTAL'], rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pipeline_waterfall.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've now practiced:\n",
    "\n",
    "1. **Data Cleaning** - Deduplication, standardization, handling missing values\n",
    "2. **SQL Queries** - Conversion rates, deal sizes, pipeline velocity\n",
    "3. **Conversion Analysis** - Segmentation, identifying best/worst performers\n",
    "4. **Attribution Models** - First-touch, last-touch, linear, time-decay\n",
    "5. **Attribution Comparison** - Understanding model trade-offs\n",
    "6. **Dark Funnel Analysis** - Self-reported vs tracked attribution\n",
    "7. **Cohort Analysis** - Tracking conversion over time\n",
    "8. **Pipeline Reporting** - Sourced vs influenced pipeline\n",
    "9. **Incrementality Testing** - Holdout test design\n",
    "10. **Visualization** - Funnels, comparisons, waterfalls\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply these techniques to your actual company data\n",
    "- Build automated reports using these patterns\n",
    "- Experiment with different attribution models to see which tells the most useful story for your business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all dataframes for future use\n",
    "leads_clean.to_csv('leads_clean.csv', index=False)\n",
    "accounts_df.to_csv('accounts.csv', index=False)\n",
    "touches_df.to_csv('touches.csv', index=False)\n",
    "opportunities_df.to_csv('opportunities.csv', index=False)\n",
    "contact_to_account_df.to_csv('contact_to_account.csv', index=False)\n",
    "\n",
    "print(\"Data saved to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
